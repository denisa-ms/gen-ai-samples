{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI logprobs parameters to assess acuracy for classification of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, HTML, JSON, Markdown, Image\n",
    "import textwrap\n",
    "import json \n",
    "\n",
    "load_dotenv()\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_GPT4_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_GPT4_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = AZURE_OPENAI_ENDPOINT, \n",
    "  api_key=AZURE_OPENAI_API_KEY,  \n",
    "  api_version=AZURE_OPENAI_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openAI(text):\n",
    "    system_message = \"\"\"\n",
    "    Please read the following CONTEXT and generate two question and answer json objects in an array based on the CONTEXT provided. \n",
    "    The questions should require deep reading comprehension, logical inference, deduction, and connecting ideas across the text. \n",
    "    Avoid simplistic retrieval or pattern matching questions. Instead, focus on questions that test the ability to reason about the text in complex ways, draw subtle conclusions, and combine multiple pieces of information to arrive at an answer. Ensure that the questions are relevant, specific, and cover the key points of the CONTEXT.  Provide concise answers to each question, directly quoting the text from provided context. Provide the array output in strict JSON format as shown in output format. Ensure that the generated JSON is 100 percent structurally correct, with proper nesting, comma placement, and quotation marks. There should not be any comma after last element in the array.\n",
    "  \n",
    "    Output format:\n",
    "    [\n",
    "    {\n",
    "        \"question\": \"Question 1\",\n",
    "        \"answer\": \"Answer 1\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Question 2\",\n",
    "        \"answer\": \"Answer 2\"\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    CONTEXT:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.completions.create(\n",
    "        model=AZURE_OPENAI_GPT4_DEPLOYMENT_NAME,\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":system_message},\n",
    "            {\"role\":\"user\",\"content\":text}\n",
    "            ],\n",
    "        temperature=1\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4\",\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION_PROMPT = \"\"\"You will be given a headline of a news article.\n",
    "Classify the article into one of the following categories: Technology, Politics, Sports, and Art.\n",
    "Return only the name of the category, and nothing else.\n",
    "MAKE SURE your output is one of the four categories stated.\n",
    "Article headline: {headline}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = [\n",
    "    \"Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\",\n",
    "    \"Local Mayor Launches Initiative to Enhance Urban Public Transport.\",\n",
    "    \"Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\",\n",
    "]\n",
    "for headline in headlines:\n",
    "    print(f\"\\nHeadline: {headline}\")\n",
    "    API_RESPONSE = get_completion([{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],model=AZURE_OPENAI_GPT4_DEPLOYMENT_NAME,)\n",
    "    print(f\"Category: {API_RESPONSE.choices[0].message.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> Technology, <span style='color: darkorange'>logprobs:</span> -1.9361265e-07, <span style='color: cyan'>Output token 2:</span>  Technology, <span style='color: darkorange'>logprobs:</span> -16.39301, "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Headline: Local Mayor Launches Initiative to Enhance Urban Public Transport.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> Politics, <span style='color: darkorange'>logprobs:</span> -1.9361265e-07, <span style='color: cyan'>Output token 2:</span>  Politics, <span style='color: darkorange'>logprobs:</span> -16.440653, "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Headline: Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> Art, <span style='color: darkorange'>logprobs:</span> -0.017801497, <span style='color: cyan'>Output token 2:</span> Sports, <span style='color: darkorange'>logprobs:</span> -4.037363, "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's rerun the same prompt but with logprobs enabled, and top_logprobs set to 2 \n",
    "# (this will show us the 2 most likely output tokens for each token)\n",
    "for headline in headlines:\n",
    "    print(f\"\\nHeadline: {headline}\")\n",
    "    API_RESPONSE = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n",
    "        model=\"gpt-4\",\n",
    "        logprobs=True,\n",
    "        top_logprobs=2,\n",
    "    )\n",
    "    top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs\n",
    "    html_content = \"\"\n",
    "    for i, logprob in enumerate(top_two_logprobs, start=1):\n",
    "        html_content += (\n",
    "            f\"<span style='color: cyan'>Output token {i}:</span> {logprob.token}, \"\n",
    "            f\"<span style='color: darkorange'>logprobs:</span> {logprob.logprob}, \"\n",
    "        )\n",
    "    display(HTML(html_content))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
