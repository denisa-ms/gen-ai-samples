{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-4o; API Version:2024-10-21\n",
      "Azure OpenAI model is ready to use!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "AZURE_OPENAI_ENDPOINT=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_KEY=os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_API_VERSION=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_GPT4o_DEPLOYMENT=os.getenv(\"AZURE_OPENAI_GPT4o_DEPLOYMENT\")\n",
    "\n",
    "#init the openai client\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = AZURE_OPENAI_ENDPOINT, \n",
    "  api_key=AZURE_OPENAI_KEY,  \n",
    "  api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "print(f\"Model: {AZURE_OPENAI_GPT4o_DEPLOYMENT}; API Version:{AZURE_OPENAI_API_VERSION}\")\n",
    "print(\"Azure OpenAI model is ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Service in australiaeast is ready to use!\n"
     ]
    }
   ],
   "source": [
    "SPEECH_KEY = os.getenv(\"SPEECH_KEY\")\n",
    "SPEECH_REGION = os.getenv(\"SPEECH_REGION\")\n",
    "engine_name = \"test\"\n",
    "\n",
    "print(f\"Speech Service in {SPEECH_REGION} is ready to use!\")\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)\n",
    "# Set up Azure Text-to-Speech language \n",
    "speech_config.speech_synthesis_language = \"en-US\"\n",
    "# Set up Azure Speech-to-Text language recognition\n",
    "speech_config.speech_recognition_language = \"en-US\"\n",
    "# Use an absolute path for the log file\n",
    "log_file_path = os.path.abspath(\"./log/log.txt\")\n",
    "speech_config.set_property(speechsdk.PropertyId.Speech_LogFilename, log_file_path)\n",
    "\n",
    "\n",
    "# Set up the voice configuration\n",
    "speech_config.speech_synthesis_voice_name = \"en-US-JennyMultilingualNeural\"\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the speech-to-text function using continuous recognition\n",
    "def speech_to_text():\n",
    "    # Set up the audio configuration\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    # Create a speech recognizer and start the recognition\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Say something...\")\n",
    "\n",
    "    done = False\n",
    "    all_results = []\n",
    "\n",
    "    def stop_cb(evt):\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    def recognized_cb(evt):\n",
    "        if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "            print(\"Recognized: {}\".format(evt.result.text))\n",
    "            all_results.append(evt.result.text)\n",
    "        elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "            print(\"No speech could be recognized: {}\".format(evt.result.no_match_details))\n",
    "        return \"\"\n",
    "\n",
    "    speech_recognizer.recognized.connect(recognized_cb)\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        pass\n",
    "\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "    return \" \".join(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text-to-speech function\n",
    "def text_to_speech(text):\n",
    "    try:\n",
    "        result = speech_synthesizer.speak_text_async(text).get()\n",
    "        if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "            print(\"Text-to-speech conversion successful.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error synthesizing audio: {result}\")\n",
    "            return False\n",
    "    except Exception as ex:\n",
    "        print(f\"Error synthesizing audio: {ex}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentiment(text):\n",
    "    system_message = \"\"\"\n",
    "    You are an AI assistant that helps recognize the sentiment in a given text.\n",
    "    1. Evaluate the given text and provide the category of the sentiment as either positive, negative, or neutral.\n",
    "    2. Do not provide any additional examples to the output, just the category.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_GPT4o_DEPLOYMENT,\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":system_message},\n",
    "            {\"role\":\"user\",\"content\":text}\n",
    "            ],\n",
    "        temperature=0   \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_from_file_short(filename):\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=filename)\n",
    "    # Creates a speech recognizer using a file as audio input, also specify the speech language\n",
    "    speech_config.speech_recognition_language = \"en-US\"\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "        speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # Starts speech recognition, and returns after a single utterance is recognized. The end of a\n",
    "    # single utterance is determined by listening for silence at the end or until a maximum of about 30\n",
    "    # seconds of audio is processed. It returns the recognition text as result.\n",
    "    # Note: Since recognize_once() returns only a single utterance, it is suitable only for single\n",
    "    # shot recognition like command or query.\n",
    "    # For long-running multi-utterance recognition, use start_continuous_recognition() instead.\n",
    "    result = speech_recognizer.recognize_once()\n",
    "\n",
    "    if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        return result.text\n",
    "    elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(result.no_match_details))\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For long-running multi-utterance recognition, use start_continuous_recognition() instead.\n",
    "def speech_from_file_full(filename):\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=filename)\n",
    "    speech_config.speech_recognition_language = \"en-US\"\n",
    "    \n",
    "    # Creates a speech recognizer using a file as audio input\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # List to store the recognized text\n",
    "    all_results = []\n",
    "\n",
    "    def recognized_cb(evt):\n",
    "        if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "            print(\"Recognized: {}\".format(evt.result.text))\n",
    "            all_results.append(evt.result.text)\n",
    "        elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "            print(\"No speech could be recognized: {}\".format(evt.result.no_match_details))\n",
    "\n",
    "    def stop_cb(evt):\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    # Connect callbacks to the events fired by the speech recognizer\n",
    "    speech_recognizer.recognized.connect(recognized_cb)\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # Start continuous recognition\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        pass\n",
    "\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "    # Return the concatenated results\n",
    "    return \" \".join(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_GPT4o_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "             Extract the entities from the text and provide only JSON output.\n",
    "             \n",
    "             {caller_name: \"John\", call_purpose: \"meeting\"}\n",
    "\n",
    "             \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, target_language):\n",
    "    system_message = \"\"\"You are a helpful assistant that translates text into \"\"\" + target_language + \"\"\".\n",
    "    Answer in a clear and concise manner only translating the text. \n",
    "    Ignore filler words like 'ng', 'uh', etc. and any unnatural sentence breaks. \n",
    "    Ensure the translation flows smoothly and natually while preserving the intended meaning.\n",
    "    Text:\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_GPT4o_DEPLOYMENT,\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":system_message},\n",
    "            {\"role\":\"user\",\"content\":text}\n",
    "            ],\n",
    "        temperature=0   \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full transcription\n",
    "source = \"./data/Call1_separated_16k_health_insurance.wav\"\n",
    "transcription_full = speech_from_file_full(source)\n",
    "print(f\"Transcription: {transcription_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the text to Chinese using OpenAI\n",
    "translated_text_full = translate(transcription_full, \"Chinese\")\n",
    "print(f\"Translated to Chinese: {translated_text_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the response to speech using text-to-speech\n",
    "# In Azure Speech, Speech Synthesis Markup Language (SSML) can be used to specifu different voices.\n",
    "text_to_speech(translated_text_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: Hello, thank you for calling Contoso, who am I speaking with today? Hi, my name is Mary Rondo. I'm trying to enroll myself with Contuso. Hi Mary. Uh, are you calling because you need health insurance? Yes, Yeah, I'm calling to sign up for insurance. Great. Uh, if you can answer a few questions, uh, we can get you signed up in a jiffy. OK. Umm, So, uh, what's your full name?\n"
     ]
    }
   ],
   "source": [
    "#The speech_from_file_short function use recognize_once() method. It is designed for short, single utterances and has a default limit of about 30 seconds for demo purpose.\n",
    "source = \"./data/Call1_separated_16k_health_insurance.wav\"\n",
    "transcription_short = speech_from_file_short(source)\n",
    "print(f\"Transcription: {transcription_short}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Neutral\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the sentiment using OpenAI\n",
    "response = evaluate_sentiment(transcription_short)\n",
    "print(f\"Sentiment: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: {\n",
      "  \"caller_name\": \"Mary Rondo\",\n",
      "  \"call_purpose\": \"sign up for insurance\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "entities = extract_entities(transcription_short)\n",
    "print(f\"Entities: {entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated to Chinese: 您好，感谢您致电Contoso，请问我今天在和谁通话？您好，我叫玛丽·隆多。我想注册Contoso。您好，玛丽。请问您是因为需要健康保险而打电话吗？是的，我打电话是为了注册保险。好的，如果您能回答几个问题，我们可以很快为您注册。好的。那么，您的全名是什么？\n"
     ]
    }
   ],
   "source": [
    "# Translate the text to Chinese using OpenAI\n",
    "translated_text_short_cn = translate(transcription_short, \"Chinese\")\n",
    "print(f\"Translated to Chinese: {translated_text_short_cn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated to Chinese: こんにちは、コンソーにお電話いただきありがとうございます。今日はどなたとお話ししていますか？こんにちは、私の名前はメアリー・ロンドです。コンソーに登録しようとしています。こんにちは、メアリー。健康保険が必要でお電話いただいたのですか？はい、保険に加入するために電話しました。素晴らしいです。いくつか質問にお答えいただければ、すぐに登録できます。わかりました。では、フルネームを教えてください。\n"
     ]
    }
   ],
   "source": [
    "# Translate the text to Japanese using OpenAI\n",
    "translated_text_short_jp = translate(transcription_short, \"Japanese\")\n",
    "print(f\"Translated to Chinese: {translated_text_short_jp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-speech conversion successful.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the response to speech using text-to-speech\n",
    "text_to_speech(translated_text_short_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-speech conversion successful.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the response to speech using text-to-speech\n",
    "text_to_speech(translated_text_short_jp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
