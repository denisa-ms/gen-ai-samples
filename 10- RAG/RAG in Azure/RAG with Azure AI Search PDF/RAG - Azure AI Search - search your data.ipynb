{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Azure AI search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from openai import AzureOpenAI\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIParameters\n",
    ")\n",
    "\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment variables\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "\n",
    "AZURE_OPENAI_EMBEDDINGS_ADA_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_ADA_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION=\"2024-02-01\"\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AISTUDIO_OPENAI_GPT4_DEPLOYMENT_NAME = os.getenv(\"AISTUDIO_OPENAI_GPT4_DEPLOYMENT_NAME\")\n",
    "azure_openai_embedding_dimensions = 1536\n",
    "index_name = \"books\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI API\n",
    "aoai_client = AzureOpenAI(\n",
    "  azure_endpoint = AZURE_OPENAI_ENDPOINT, \n",
    "  api_key=AZURE_OPENAI_API_KEY,  \n",
    "  api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "credential = AzureKeyCredential(key)\n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Generate Document Embeddings using OpenAI Ada Model\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "def calc_embeddings(text):\n",
    "    # model = \"deployment_name\"\n",
    "    embeddings = aoai_client.embeddings.create(input = [text], model=AZURE_OPENAI_EMBEDDINGS_ADA_DEPLOYMENT_NAME).data[0].embedding\n",
    "    return embeddings\n",
    "\n",
    "def do_search(query):\n",
    "    fields = \"embedding\"\n",
    "    embedding = calc_embeddings(query)\n",
    "    vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=fields)\n",
    "  \n",
    "    results = search_client.search(  \n",
    "        search_text=None,  \n",
    "        vector_queries= [vector_query],\n",
    "        select=[\"content\"],\n",
    "    )  \n",
    "    answer = ''\n",
    "    for result in results:  \n",
    "        print(f\"Score: {result['@search.score']}\")  \n",
    "        print(f\"Content: {result['content']}\")  \n",
    "        answer = answer + result['content']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.878709\n",
      "Content: the studded iron-bound cask followed the sailor to the bottom, as if to \n",
      "yield him his pillow, though in sooth but a hard one. \n",
      " \n",
      "And thus the first man of the Pequod that mounted the mast to look out \n",
      "for the White Whale, on the White Whale’s own peculiar ground; that man \n",
      "was swallowed up in the deep. But few, perhaps, thought of that at the \n",
      "time. Indeed, in some sort, they were not grieved at this event, at \n",
      "least as a portent; for they regarded it, not as a foreshadowing of \n",
      "evil in the future, but as the fulfilment of an evil already presaged. \n",
      "They declared that now they knew the reason of those wild shrieks they \n",
      "had heard the night before. But again the old Manxman said nay. \n",
      " \n",
      "The lost life-buoy was now to be replaced; Starbuck was directed to see \n",
      "to it; but as no cask of sufficient lightness could be found, and as in \n",
      "the feverish eagerness of what seemed the approaching crisis of the \n",
      "voyage, all hands were impatient of any toil but what was directly\n",
      "Score: 0.8743553\n",
      "Content: now’s the very dreaded symbol of grim death, by a mere hap, made the \n",
      "expressive sign of the help and hope of most endangered life. A \n",
      "life-buoy of a coffin! Does it go further? Can it be that in some \n",
      "spiritual sense the coffin is, after all, but an immortality-preserver! \n",
      "I’ll think of that. But no. So far gone am I in the dark side of earth, \n",
      "that its other side, the theoretic bright one, seems but uncertain \n",
      "twilight to me. Will ye never have done, Carpenter, with that accursed \n",
      "sound? I go below; let me not see that thing here when I return again. \n",
      "Now, then, Pip, we’ll talk this over; I do suck most wondrous \n",
      "philosophies from thee! Some unknown conduits from the unknown worlds \n",
      "must empty into thee!” \n",
      " \n",
      " \n",
      "CHAPTER 128. The Pequod Meets The Rachel. \n",
      " \n",
      "Next day, a large ship, the Rachel, was descried, bearing directly down\n",
      "Score: 0.8743217\n",
      "Content: connected with its final end, whatever that might prove to be; \n",
      "therefore, they were going to leave the ship’s stern unprovided with a \n",
      "buoy, when by certain strange signs and inuendoes Queequeg hinted a \n",
      "hint concerning his coffin. \n",
      " \n",
      "“A life-buoy of a coffin!” cried Starbuck, starting. \n",
      " \n",
      "“Rather queer, that, I should say,” said Stubb. \n",
      " \n",
      "“It will make a good enough one,” said Flask, “the carpenter here can \n",
      "arrange it easily.” \n",
      " \n",
      "“Bring it up; there’s nothing else for it,” said Starbuck, after a \n",
      "melancholy pause. “Rig it, carpenter; do not look at me so—the coffin, \n",
      "I mean. Dost thou hear me? Rig it.” \n",
      " \n",
      "“And shall I nail down the lid, sir?” moving his hand as with a hammer. \n",
      " \n",
      "“Aye.” \n",
      " \n",
      "“And shall I caulk the seams, sir?” moving his hand as with a \n",
      "caulking-iron. \n",
      " \n",
      "“Aye.” \n",
      " \n",
      "“And shall I then pay over the same with pitch, sir?” moving his hand \n",
      "as with a pitch-pot. \n",
      " \n",
      "“Away! what possesses thee to this? Make a life-buoy of the coffin, and\n"
     ]
    }
   ],
   "source": [
    "question = \"Why does the coffin prepared for Queequeg become Ishmael's life buoy once the Pequod sinks?\"\n",
    "answers = do_search(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openAI(text):\n",
    "    response = aoai_client.chat.completions.create(\n",
    "        model=AISTUDIO_OPENAI_GPT4_DEPLOYMENT_NAME,\n",
    "        messages = text,\n",
    "        temperature=0.7,\n",
    "        max_tokens=800,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- The lost life-buoy was to be replaced, but no sufficiently light cask could be found.\\n- Queequeg hinted his coffin could be used as a life-buoy.\\n- Starbuck directed the carpenter to rig the coffin as a life-buoy.\\n- The coffin was prepared and used as a life-buoy because there was no other option available.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This prompt provides instructions to the model. \n",
    "# The prompt includes the query and the source, which are specified further down in the code.\n",
    "grounded_prompt=\"\"\"\n",
    "You are a friendly assistant answering users questions.\n",
    "Answer the query using only the answers provided below in a friendly and concise bulleted manner.\n",
    "Answer ONLY with the facts listed in the list of answers below.\n",
    "If there isn't enough information below, say you don't know.\n",
    "Do not generate answers that don't use the answers below.\n",
    "Query: {question}\n",
    "Sources:\\n{answers}\n",
    "\"\"\"\n",
    "# prepare prompt\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": grounded_prompt.format(question=question, answers=answers)\n",
    "    }\n",
    "]\n",
    "  \n",
    "result = call_openAI(messages)\n",
    "display(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
