{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from openai import AzureOpenAI\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIParameters\n",
    ")\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment variables\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "\n",
    "AZURE_OPENAI_EMBEDDINGS_ADA_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_ADA_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_GPT4_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_GPT4_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_embedding_dimensions = 1536\n",
    "index_name = \"legal_clauses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI API\n",
    "aoai_client = AzureOpenAI(\n",
    "  azure_endpoint = AZURE_OPENAI_ENDPOINT, \n",
    "  api_key=AZURE_OPENAI_API_KEY,  \n",
    "  api_version=\"2023-05-15\"\n",
    ")\n",
    "credential = AzureKeyCredential(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Document Embeddings using OpenAI Ada Model\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "def calc_embeddings(text):\n",
    "    # model = \"deployment_name\"\n",
    "    embeddings = aoai_client.embeddings.create(input = [text], model=AZURE_OPENAI_EMBEDDINGS_ADA_DEPLOYMENT_NAME).data[0].embedding\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# splitting into 1000 char long chunks with 30 char overlap\n",
    "# split [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=30,\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(columns=['id', 'document_name', 'content', 'embedding'])\n",
    " \n",
    "id = 0 \n",
    "for entry in os.scandir(\"./data/clauses/\"):\n",
    "    if entry.is_file():\n",
    "        print(entry.name)\n",
    "        filename = entry.name\n",
    "        full_path = entry.path\n",
    "        loader = PyPDFLoader(full_path)\n",
    "        pages = loader.load_and_split(text_splitter=splitter)\n",
    "        for page in pages:\n",
    "            df.loc[len(df.index)] = [str(id), filename, page.page_content, \"\"]  \n",
    "            id += 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the embeddings using openAI ada \n",
    "df[\"embedding\"] = df.content.apply(lambda x: calc_embeddings(x))\n",
    "df.to_csv('./data/clauses_with_embeddings.csv', index=False)\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output embeddings to json file\n",
    "output_path = os.path.join('.', 'data', 'clauses_with_embeddings.json')\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    df.to_json(f, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search index\n",
    "index_client = SearchIndexClient(endpoint=service_endpoint, credential=credential)\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"document_name\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"embedding\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\")\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "            vectorizer=\"myVectorizer\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            name=\"myVectorizer\",\n",
    "            azure_open_ai_parameters=AzureOpenAIParameters(\n",
    "                resource_uri=AZURE_OPENAI_ENDPOINT,\n",
    "                deployment_id=AZURE_OPENAI_EMBEDDINGS_ADA_DEPLOYMENT_NAME,\n",
    "                model_name=AZURE_OPENAI_EMBEDDINGS_ADA_DEPLOYMENT_NAME,\n",
    "                api_key=AZURE_OPENAI_API_KEY\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"content\")\n",
    "    )\n",
    ")\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "index_name = \"legal_clauses\"\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "import json\n",
    "\n",
    "# Upload some documents to the index\n",
    "output_path = os.path.join('.', 'data', 'clauses_with_embeddings.json')\n",
    "with open(output_path, 'r') as file:  \n",
    "    documents = json.load(file)  \n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)\n",
    "print(f\"Uploaded {len(documents)} documents\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "def do_search(query, fields):\n",
    "    embedding = calc_embeddings(query)\n",
    "    vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=fields)\n",
    "  \n",
    "    results = search_client.search(  \n",
    "        search_text=None,  \n",
    "        vector_queries= [vector_query],\n",
    "        select=[\"content\", \"document_name\"],\n",
    "    )  \n",
    "\n",
    "    for result in results:  \n",
    "        print(f\"Score: {result['@search.score']}\")  \n",
    "        print(f\"Clause name: {result['document_name']}\")  \n",
    "        print(f\"content: {result['content']}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for clause matching for incoming documents\n",
    "# splitting into 1000 char long chunks with 30 char overlap\n",
    "from azure.search.documents import SearchClient\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=30,\n",
    ")\n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "for entry in os.scandir(\"./data/documents/\"):\n",
    "    if entry.is_file():\n",
    "        filename = entry.name\n",
    "        full_path = entry.path\n",
    "        loader = PyPDFLoader(full_path)\n",
    "        pages = loader.load_and_split(text_splitter=splitter)\n",
    "        for page in pages:\n",
    "            # calculate the embeddings using openAI ada\n",
    "            do_search(page.page_content, \"embedding\")\n",
    "            print(f\"Results for {filename}:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
