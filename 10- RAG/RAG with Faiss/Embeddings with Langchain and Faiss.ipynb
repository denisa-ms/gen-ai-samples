{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Environment variables from .env file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_GPT4_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_GPT4_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_EMBEDDINGS_ADA_DEPLOYMENT_NAME  = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_ADA_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(model=AZURE_OPENAI_GPT4_DEPLOYMENT_NAME,\n",
    "             deployment_name=AZURE_OPENAI_GPT4_DEPLOYMENT_NAME,\n",
    "             openai_api_version=AZURE_OPENAI_API_VERSION,\n",
    "             temperature=0,\n",
    "             max_tokens=400\n",
    "             ):\n",
    "\n",
    "    llm = AzureChatOpenAI(deployment_name=deployment_name,\n",
    "                            model=model,\n",
    "                            openai_api_version=openai_api_version,\n",
    "                            azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "                            temperature=temperature,\n",
    "                            max_tokens=max_tokens\n",
    "                            )\n",
    "    return llm\n",
    "\n",
    "llm = init_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=AZURE_OPENAI_EMBEDDINGS_ADA_DEPLOYMENT_NAME,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    openai_api_version=AZURE_OPENAI_API_VERSION,\n",
    "    chunk_size = 1\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run ONLY ONCE to create the embeddings - Split text into chunks** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages:  292\n"
     ]
    }
   ],
   "source": [
    "fileName = \"./data/fabric-data-engineering.pdf\"\n",
    "loader = PyPDFLoader(fileName)\n",
    "pages = loader.load_and_split()\n",
    "print(\"Number of pages: \", len(pages))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run ONLY ONCE to create the embeddings - Create embeddings and save to FAISS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents=pages, embedding=embeddings)\n",
    "# save the FAISS index to disk\n",
    "db.save_local(\"./dbs/documentation/faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize retrieval API WITH your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vector store to memory\n",
    "vectorStore = FAISS.load_local(\"./dbs/documentation/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = vectorStore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})  # returns 2 most similar vectors/documents\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ask questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "No, you cannot use a Power BI datamart for a data volume of 110 GB, as Power BI datamarts are limited to data volumes up to 100 GB.\n",
       "\n",
       "For data volumes of 110 GB, you have other options available in Microsoft Fabric, such as a data warehouse or a lakehouse. Both of these options support unlimited data volumes. Here’s a brief overview of each:\n",
       "\n",
       "1. **Data Warehouse**: This is suitable for structured data and is ideal if the primary developers are familiar with SQL. It organizes data into databases, schemas, and tables, and supports read operations via Spark and T-SQL, with write operations handled by T-SQL.\n",
       "\n",
       "2. **Lakehouse**: This option can handle unstructured, semi-structured, and structured data, making it versatile for various data types. It is particularly suitable for data engineers and data scientists who are comfortable with Spark (Scala, PySpark, Spark SQL, R). Data in a lakehouse is organized in folders and files, as well as databases and tables. It supports both read and write operations via Spark.\n",
       "\n",
       "Choosing between a data warehouse and a lakehouse would depend on the type of data you have and the skill set of your development team."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = qa.invoke({\"query\": \"Can I use PowerBI datamart for 110 GB data volume? Explain which other options are available for data volumes of 110 GB?\"})\n",
    "\n",
    "display(HTML(r['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Rob, a data engineer, needs to store and model several terabytes of data in F abric. The\n",
       "team has a mix of PySpark and T-SQL skills. Most of the team running T-SQL queries are\n",
       "consumers, and therefore don't need to write INSER T, UPD ATE, or DELETE statements.\n",
       "The remaining developers are comfortable working in notebooks, and because the data\n",
       "is stored in Delta, they're able to interact with a similar SQL syntax.\n",
       "Rob decides to use a lakehouse , which allows the data engineering team to use their\n",
       "diverse skills against the data, while allowing the team members who are highly skilled\n",
       "in T-SQL to consume the data.\n",
       "Ash, a citizen developer, is a P ower BI developer. They're familiar with Excel, P ower BI,\n",
       "and Office. They need to build a data product for a business unit. They know they don't\n",
       "quite have the skills to build a data warehouse or a lakehouse, and those seem like too\n",
       "much for their needs and data volumes. They review the details in the previous table\n",
       "and see that the primary decision points are their own skills and their need for a self\n",
       "service, no code capability, and data volume under 100 GB.\n",
       "Ash works with business analysts familiar with P ower BI and Microsoft Office, and knows\n",
       "that they already have a Premium capacity subscription. As they think about their larger\n",
       "team, they realize the primary consumers of this data may be analysts, familiar with no-\n",
       "code and SQL analytical tools. Ash decides to use a Power BI datamar t, which allows the\n",
       "team to interact build the capability fast, using a no-code experience. Queries can be\n",
       "executed via P ower BI and T-SQL, while also allowing any Spark users in the organization\n",
       "to access the data as well.\n",
       "What is data warehousing in Microsoft F abric?\n",
       "Create a warehouse in Microsoft F abric\n",
       "Create a lakehouse in Microsoft F abric\n",
       "Introduction to P ower BI datamartsScenario 2\n",
       "Scenario 3\n",
       "Next steps"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Microsoft Fabric decision guide: data\n",
       "warehouse or lakehouse\n",
       "Article •05/23/2023\n",
       "Use this reference guide and the example scenarios to help you choose between the\n",
       "data warehouse or a lakehouse for your workloads using Microsoft F abric.\n",
       "Data war ehouse Lakehouse Power BI\n",
       "Datamar t\n",
       "Data v olume Unlimited Unlimited Up to 100\n",
       "GB\n",
       "Type o f data Structured Unstructured,  \n",
       "semi-structured,  \n",
       "structuredStructured\n",
       "Primar y\n",
       "developer\n",
       "personaData warehouse developer,  \n",
       "SQL engineerData engineer,  \n",
       "data scientistCitizen\n",
       "developer\n",
       "Primar y\n",
       "developer skill\n",
       "setSQL Spark  \n",
       "(Scala, PySpark, Spark SQL, R)No code,\n",
       "SQL\n",
       "Data or ganized\n",
       "byDatabases, schemas, and\n",
       "tablesFolders and files,  \n",
       "databases and tablesDatabase,\n",
       "tables,\n",
       "queries\n",
       "Read\n",
       "operationsSpark,  \n",
       "T-SQLSpark,  \n",
       "T-SQLSpark,  \n",
       "T-SQL,  \n",
       "Power BI\n",
       "Write\n",
       "operationsT-SQL Spark  \n",
       "(Scala, PySpark, Spark SQL, R)Dataflows,\n",
       "T-SQL） Impor tant\n",
       "Microsoft F abric is currently in PREVIEW. This information relates to a prerelease\n",
       "product that may be substantially modified before it's released. Microsoft makes no\n",
       "warranties, expressed or implied, with respect to the information provided here.\n",
       "Data warehouse and lakehouse properties"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for document in r['source_documents']:\n",
    "    display(HTML(document.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "To load a CSV file to a Delta table in Microsoft Fabric, you can follow these steps:\n",
       "\n",
       "1. Open Microsoft Fabric and select the Synapse Data Engineering experience.\n",
       "2. Make sure you are in the desired workspace or select/create one.\n",
       "3. Click on the Lakehouse icon under the New section in the main image.\n",
       "4. Select the Files section and choose the CSV file you want to upload.\n",
       "5. Once the file is selected, it will be loaded into a new Delta table in the Tables section.\n",
       "6. If the table already exists, it will be dropped and then created.\n",
       "7. The CSV file will be converted to a Delta table format.\n",
       "8. You can now generate a dataset and create a Power BI report using the Delta table.\n",
       "\n",
       "Please note that Microsoft Fabric is currently in preview, and the steps may vary slightly as the product evolves."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = qa.invoke({\"query\": \"What are the steps to load a CSV file to a delta table in Microsoft Fabric?\"})\n",
    "\n",
    "display(HTML(r['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "A file selected in the Lakehouse Filessection to be loaded into a new Delta table in the\n",
       "Tables section. If the table already exists, it is dropped and then created.\n",
       "What is Delta Lake?\n",
       "CSV file upload to Delta for P ower BI reporting\n",
       "\n",
       "Next steps"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "CSV file upload to Delta for Power BI\n",
       "reporting\n",
       "Article •05/23/2023\n",
       "Microsoft F abric Lakehouse  is a data architecture platform for storing, managing, and\n",
       "analyzing structured and unstructured data in a single location.\n",
       "In this tutorial you learn to:\n",
       "Upload a CSV file to a Lakehouse\n",
       "Convert the file to a Delta table\n",
       "Generate a Dataset and create a P ower BI report\n",
       "1. In Microsoft F abric, select Synapse Data Engineering  experience\n",
       "2. Make sure you are in desired workspace or select/create one\n",
       "3. Select Lakehouse  icon under New section in the main mage） Impor tant\n",
       "Microsoft F abric is currently in PREVIEW. This information relates to a prerelease\n",
       "product that may be substantially modified before it's released. Microsoft makes no\n",
       "warranties, expressed or implied, with respect to the information provided here.\n",
       "Create a Lakehouse and get a CSV file ready"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for document in r['source_documents']:\n",
    "    display(HTML(document.page_content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
