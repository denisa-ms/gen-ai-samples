{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Azure Data Explorer  \n",
    "\n",
    "You can run this notebook after running succesfully the \"RAG - ADX - create embeddings\" notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "import os\n",
    "\n",
    "# Configure environment variables\n",
    "load_dotenv()\n",
    "\n",
    "AAD_TENANT_ID = os.getenv(\"AAD_TENANT_ID\")\n",
    "KUSTO_CLUSTER = os.getenv(\"KUSTO_CLUSTER\")\n",
    "KUSTO_DATABASE = os.getenv(\"KUSTO_DATABASE\")\n",
    "KUSTO_TABLE = os.getenv(\"KUSTO_TABLE\")\n",
    "KUSTO_MANAGED_IDENTITY_APP_ID = os.getenv(\"KUSTO_MANAGED_IDENTITY_APP_ID\")\n",
    "KUSTO_MANAGED_IDENTITY_SECRET = os.getenv(\"KUSTO_MANAGED_IDENTITY_SECRET\")\n",
    "\n",
    "# Configure OpenAI API\n",
    "OPENAI_GPT35_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT35_DEPLOYMENT_NAME\")\n",
    "OPENAI_GPT4_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT4_DEPLOYMENT_NAME\")\n",
    "OPENAI_GPT4V_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT4V_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.kusto.data import KustoClient, KustoConnectionStringBuilder\n",
    "from azure.kusto.data.exceptions import KustoServiceError\n",
    "from azure.kusto.data.helpers import dataframe_from_result_table\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPT to answer a question based on the question and the answer from our similarity search\n",
    "from openai import AzureOpenAI\n",
    "clientOpenAI = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_DEPLOYMENT_ENDPOINT, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "def call_openAI(text):\n",
    "    response = clientOpenAI.chat.completions.create(\n",
    "        model=OPENAI_GPT35_DEPLOYMENT_NAME,\n",
    "        messages = text,\n",
    "        temperature=0.7,\n",
    "        max_tokens=800,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingmodel = AzureOpenAIEmbeddings(\n",
    "    deployment=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    model=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "    chunk_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to adx using AAD app registration\n",
    "cluster = KUSTO_CLUSTER\n",
    "kcsb = KustoConnectionStringBuilder.with_aad_application_key_authentication(cluster, KUSTO_MANAGED_IDENTITY_APP_ID, KUSTO_MANAGED_IDENTITY_SECRET,  AAD_TENANT_ID)\n",
    "print(kcsb)\n",
    "client = KustoClient(kcsb)\n",
    "kusto_db = KUSTO_DATABASE\n",
    "table_name = \"embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title :The Projec\n",
      "Title :CONTENTS  \n"
     ]
    }
   ],
   "source": [
    "#testing the connection to kusto works - sample query to get the top 10 results from wikipedia\n",
    "query = table_name + \" | take 2\"\n",
    "\n",
    "response = client.execute(kusto_db, query)\n",
    "for row in response.primary_results[0]:\n",
    "    txt = (row[\"content\"])[0:10]\n",
    "    print(\"Title :{}\".format(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the tenacity library to create delays and retries when calling openAI embeddings to avoid hitting throttling limits\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def calc_embeddings(text):\n",
    "    deployment = OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\n",
    "    # replace newlines, which can negatively affect performance.\n",
    "    txt = text.replace(\"\\n\", \" \")\n",
    "    return embeddingmodel.embed_query(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_adx(question, nr_of_answers=1):\n",
    "        searchedEmbedding = calc_embeddings(question)\n",
    "        kusto_query = KUSTO_TABLE + \" | extend similarity = series_cosine_similarity(dynamic(\"+str(searchedEmbedding)+\"), embedding) | top \" + str(nr_of_answers) + \" by similarity desc \"\n",
    "        response = client.execute(kusto_db, kusto_query)\n",
    "\n",
    "        for row in response.primary_results[0]:\n",
    "                return row['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drawn from it, and then had the iron part placed in the coffin along  \n",
      "with one of the paddles of his boat. All by his own request, also,  \n",
      "biscuits were then ranged round the sides within: a flask of fresh  \n",
      "water was placed at the head, and a small bag of woody earth scraped up  \n",
      "in the hold at the foot; and a piece of sail -cloth being rolled up for  \n",
      "a pillow, Queequeg now entreated to be lifted into his final bed, that  \n",
      "he might make trial of its comforts, if any it had. He lay without  \n",
      "moving a few minutes, then told one to go to his bag and bring out his  \n",
      "little god, Yojo. Then crossing his arms on his breast with Yojo  \n",
      "between, he called for the coffin lid (hatch he called it) to be placed  \n",
      "over him. The head part turned over with a leather hinge, and there lay  \n",
      "Queequeg in his coffin with little but his composed countenance in  \n",
      "view. “Rarmai” (it will do; it is easy), he murmured at last, and  \n",
      "signed to be replaced in his hammock.\n"
     ]
    }
   ],
   "source": [
    "# this is the question we want to ask and its embeddings\n",
    "question = \"Why does the coffin prepared for Queequeg become Ishmael's life buoy once the Pequod sinks?\"\n",
    "answer_from_ADX = get_answer_from_adx(question,1)\n",
    "print(answer_from_ADX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The coffin prepared for Queequeg becomes Ishmael's life buoy once the Pequod sinks because it is filled with provisions and supplies, making it a suitable vessel to keep him afloat in the water."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = 'Question: {}'.format(question) + '\\n' + 'Information: {}'.format(answer_from_ADX)\n",
    "# prepare prompt\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users questions. Answer the question using the provided information and do not add anything else.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "result = call_openAI(messages)\n",
    "display(HTML(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Ahab pursues Moby Dick because he is consumed by the purpose of capturing the whale. He is willing to sacrifice all mortal interests for this one passion. Additionally, he may be too accustomed to the ways of a whaleman to abandon the pursuit completely."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Why does Ahab pursue Moby Dick?\"\n",
    "retrieved_answer_from_adx = get_answer_from_adx(question,1)\n",
    "\n",
    "prompt = 'Question: {}'.format(question) + '\\n' + 'Information: {}'.format(retrieved_answer_from_adx)\n",
    "\n",
    "# prepare prompt\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users questions. Answer the question using the provided information and do not add anything else.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "result = call_openAI(messages)\n",
    "display(HTML(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
