{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Azure Cosmos DB for Mongo vcore\n",
    "You can run this notebook after running succesfully the \"RAG - Cosmos for Mongo - create embeddings\" notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "import os\n",
    "\n",
    "# Configure environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure OpenAI API\n",
    "OPENAI_GPT35_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT35_DEPLOYMENT_NAME\")\n",
    "OPENAI_GPT4_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT4_DEPLOYMENT_NAME\")\n",
    "OPENAI_GPT4V_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT4V_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_DALLE_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DALLE_DEPLOYMENT_NAME\")\n",
    "\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#configure Cosmos \n",
    "COSMOS_MONGO_CONNECTION_STRING = os.getenv(\"COSMOS_MONGO_CONNECTION_STRING\")\n",
    "COSMOS_INDEX_NAME = os.getenv(\"COSMOS_INDEX_NAME\")\n",
    "COSMOS_DBNAME = os.getenv(\"COSMOS_DBNAME\")\n",
    "COSMOS_COLLECTION_NAME = os.getenv(\"COSMOS_COLLECTION_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingmodel = AzureOpenAIEmbeddings(\n",
    "    deployment=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    model=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "    chunk_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the tenacity library to create delays and retries when calling openAI embeddings to avoid hitting throttling limits\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def calc_embeddings(text):\n",
    "    deployment = OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\n",
    "    # replace newlines, which can negatively affect performance.\n",
    "    txt = text.replace(\"\\n\", \" \")\n",
    "    return embeddingmodel.embed_query(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using database: 'embed-test-db'.\n",
      "\n",
      "Using collection: 'embed-test-collection'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from langchain_community.vectorstores.azure_cosmos_db import (\n",
    "    AzureCosmosDBVectorSearch,\n",
    "    CosmosDBSimilarityType,\n",
    ")\n",
    "\n",
    "client: MongoClient = MongoClient(COSMOS_MONGO_CONNECTION_STRING)\n",
    "\n",
    "# Create database if it doesn't exist\n",
    "db = client[COSMOS_DBNAME]\n",
    "if COSMOS_DBNAME not in client.list_database_names():\n",
    "    # Create a database with 400 RU throughput that can be shared across\n",
    "    # the DB's collections\n",
    "    db = client[COSMOS_DBNAME]\n",
    "    print(\"Created db '{}'.\\n\".format(COSMOS_DBNAME))\n",
    "else:\n",
    "    print(\"Using database: '{}'.\\n\".format(COSMOS_DBNAME))\n",
    "\n",
    "# Create collection if it doesn't exist\n",
    "collection = db[COSMOS_COLLECTION_NAME]\n",
    "if COSMOS_COLLECTION_NAME not in db.list_collection_names():\n",
    "    # Creates a unsharded collection that uses the DBs shared throughput\n",
    "    collection = db[COSMOS_COLLECTION_NAME]\n",
    "    print(\"Created collection '{}'.\\n\".format(COSMOS_COLLECTION_NAME))\n",
    "else:\n",
    "    print(\"Using collection: '{}'.\\n\".format(COSMOS_COLLECTION_NAME))\n",
    "\n",
    "collection = client[COSMOS_DBNAME][COSMOS_COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I, Ishmael, should infallibly light upon, for all the world as though  \n",
      "it had turned out by chance; and in that vessel I must immediately ship  \n",
      "myself, for the present irrespective of Queequeg.  \n",
      " \n",
      "I have forgotten to mention that, in many things, Queequeg placed great  \n",
      "confidence in the excellence of Yojo’s judgment and surprising forecast  \n",
      "of things; and cherished Yojo with considerable esteem, as a rather  \n",
      "good sort of god, who perhaps meant well enough upon the whole, but in  \n",
      "all cases did not succeed in his benevolent designs.  \n",
      " \n",
      "Now, this plan of Queequeg’s, or rather Yojo’s, touching the selection  \n",
      "of our craft; I did not like that plan at all. I had not a little  \n",
      "relied upon Queequeg’s sagacity to point out the whaler best fitted to  \n",
      "carry us and our fortunes securely. But as all my remonstrances  \n",
      "produced no effect upon Queequeg, I was obliged to acquiesce; and  \n",
      "accordingly prepared to set about this business with a determined\n"
     ]
    }
   ],
   "source": [
    "vectorstore = AzureCosmosDBVectorSearch(\n",
    "    collection, embeddingmodel, index_name=COSMOS_INDEX_NAME\n",
    ")\n",
    "\n",
    "# perform a similarity search between a query and the ingested documents\n",
    "question = \"Why does the coffin prepared for Queequeg become Ishmael's life buoy once the Pequod sinks?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "answer = docs[0].page_content\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPT to answer a question based on the question and the answer from our similarity search\n",
    "from openai import AzureOpenAI\n",
    "clientOpenAI = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_DEPLOYMENT_ENDPOINT, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "def call_openAI(text):\n",
    "    response = clientOpenAI.chat.completions.create(\n",
    "        model=OPENAI_GPT35_DEPLOYMENT_NAME,\n",
    "        messages = text,\n",
    "        temperature=0.7,\n",
    "        max_tokens=800,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The coffin prepared for Queequeg becomes Ishmael's life buoy once the Pequod sinks because Ishmael has to immediately find a vessel to escape on, regardless of Queequeg. Ishmael had relied on Queequeg's judgment to choose a suitable whaler, but Queequeg's plan, influenced by his belief in Yojo's judgment, did not align with Ishmael's preferences. Therefore, Ishmael is left with no choice but to use the coffin as a life buoy and find a vessel on his own."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = 'Question: {}'.format(question) + '\\n' + 'Information: {}'.format(answer)\n",
    "# prepare prompt\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users questions. Answer the question using the provided information and do not add anything else.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "result = call_openAI(messages)\n",
    "display(HTML(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
