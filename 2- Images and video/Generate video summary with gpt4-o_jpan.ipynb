{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-4o; API Version:2024-10-21\n",
      "Azure OpenAI model is ready to use!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from IPython.display import display, HTML, JSON, Markdown, Image\n",
    "import base64 \n",
    "\n",
    "load_dotenv()\n",
    "AZURE_OPENAI_ENDPOINT=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_KEY=os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_API_VERSION=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_GPT4o_DEPLOYMENT=os.getenv(\"AZURE_OPENAI_GPT4o_DEPLOYMENT\")\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "print(f\"Model: {AZURE_OPENAI_GPT4o_DEPLOYMENT}; API Version:{AZURE_OPENAI_API_VERSION}\")\n",
    "print(\"Azure OpenAI model is ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt4o_with_imgs(image_list):\n",
    "    text=[\n",
    "    {\"role\": \"system\", \"content\": \"You are generating a video summary. Please provide a summary of the video using the frames from the video provided.\"},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        *map(lambda x: {\"type\": \"image_url\", \n",
    "                        \"image_url\": {\n",
    "                            \"url\": f'data:image/jpg;base64,{x}', \n",
    "                            \"detail\": \"low\"}}, image_list)\n",
    "        ],\n",
    "    }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_GPT4o_DEPLOYMENT,\n",
    "        messages = text,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def call_gpt4o(transcript):\n",
    "    text=[\n",
    "        {\"role\": \"system\", \"content\": \"You are generating a video summary. Please provide a summary of the video transcript.\"},\n",
    "        {\"role\": \"user\", \"content\": transcript}\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_GPT4o_DEPLOYMENT,\n",
    "        messages = text,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from moviepy import VideoFileClip\n",
    "# moviepy doc: https://zulko.github.io/moviepy/getting_started/updating_to_v2.html\n",
    "import time\n",
    "import base64\n",
    "\n",
    "# We'll be using the OpenAI DevDay Keynote Recap video. You can review the video here: https://www.youtube.com/watch?v=h02ti0Bl6zk\n",
    "VIDEO_PATH = \"data/keynote_recap.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'mp42', 'minor_version': '0', 'compatible_brands': 'isommp42', 'creation_time': '2023-12-05T19:15:46.000000Z'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1280, 720], 'bitrate': 191, 'fps': 29.97002997002997, 'codec_name': 'h264', 'profile': '(Main)', 'metadata': {'Metadata': '', 'creation_time': '2023-12-05T19:15:46.000000Z', 'handler_name': 'ISO Media file produced by Google Inc. Created on: 12/05/2023.', 'vendor_id': '[0][0][0][0]'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': 'eng', 'default': True, 'fps': 44100, 'bitrate': 127, 'metadata': {'Metadata': '', 'creation_time': '2023-12-05T19:15:46.000000Z', 'handler_name': 'ISO Media file produced by Google Inc. Created on: 12/05/2023.', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 211.0, 'bitrate': 323, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(Main)', 'video_size': [1280, 720], 'video_bitrate': 191, 'video_fps': 29.97002997002997, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 44100, 'audio_bitrate': 127, 'video_duration': 211.0, 'video_n_frames': 6323}\n",
      "c:\\Users\\jennypan\\repos\\gen-ai-samples\\venv\\Lib\\site-packages\\imageio_ffmpeg\\binaries\\ffmpeg-win-x86_64-v7.1.exe -i data/keynote_recap.mp4 -loglevel error -f image2pipe -vf scale=1280:720 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "MoviePy - Writing audio in data/keynote_recap.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Extracted 218 frames\n",
      "Extracted audio to data/keynote_recap.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def process_video(video_path, seconds_per_frame=2):\n",
    "    base64Frames = []\n",
    "    base_video_path, _ = os.path.splitext(video_path)\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    frames_to_skip = int(fps * seconds_per_frame)\n",
    "    curr_frame=0\n",
    "\n",
    "    # Loop through the video and extract frames at specified sampling rate\n",
    "    while curr_frame < total_frames - 1:\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "        curr_frame += frames_to_skip\n",
    "    video.release()\n",
    "\n",
    "    # Extract audio from video\n",
    "    audio_path = f\"{base_video_path}.mp3\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    clip.audio.write_audiofile(audio_path, bitrate=\"32k\")\n",
    "\n",
    "    print(f\"Extracted {len(base64Frames)} frames\")\n",
    "    print(f\"Extracted audio to {audio_path}\")\n",
    "    return base64Frames, audio_path\n",
    "\n",
    "# Extract 1 frame per second. You can adjust the `seconds_per_frame` parameter to change the sampling rate\n",
    "base64Frames, audio_path = process_video(VIDEO_PATH, seconds_per_frame=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Display the frames and audio for context\n",
    "display_handle = display(None, display_id=True)\n",
    "for img in base64Frames:\n",
    "    display_handle.update(Image(data=base64.b64decode(img.encode(\"utf-8\")), width=600))\n",
    "    time.sleep(0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 218\n"
     ]
    }
   ],
   "source": [
    "# print the number of frames\n",
    "print(f\"Total frames: {len(base64Frames)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Summarization\n",
    "The visual summary is generated by sending the model only the frames from the video. With just the frames, the model is likely to capture the visual aspects, but will miss any details discussed by the speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 - 0 to 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The video is a recap of the OpenAI DevDay event, showcasing the venue and the keynote presentation. It begins with the title \"OpenAI DevDay\" and \"Keynote Recap,\" followed by scenes of the event location, which is adorned with OpenAI branding. The video captures the bustling atmosphere as attendees gather in a large conference hall. The keynote presentation is highlighted, with a speaker on stage addressing the audience. The video concludes with the OpenAI DevDay logo.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2 - 20 to 39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The video appears to be a presentation or keynote event where a speaker is introducing and discussing advancements in AI technology. The main focus is on \"GPT-4 Turbo,\" an enhanced version of the GPT-4 model. The speaker is on stage, likely explaining the features and improvements of this new model. Additionally, there is a demonstration of a \"JSON Mode: ON\" feature, suggesting a technical aspect or functionality related to data handling or output format. The audience is visible, indicating a live event setting.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3 - 40 to 59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The video appears to be a presentation discussing advancements in technology, specifically focusing on JSON mode, function calling, and improvements in AI capabilities. The speaker highlights the transition from simple commands to more complex function calls, demonstrating how AI can now handle multiple tasks simultaneously. The presentation also covers six key areas of improvement, including context length, control, and better knowledge. Visuals include a JSON example, a comparison of function calling before and after improvements, and a timeline reference to September 2021, indicating a significant update or release during that time.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 - 60 to 79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The video features a presentation that took place in April 2023. The presenter, whose face is blurred, is discussing advancements in AI technology. Key topics include the introduction of DALL-E 3, GPT-4 with Turbo and Vision capabilities, and TTS (Text-to-Speech) technology. The presentation also covers the concept of Custom Models, indicating a focus on personalized or adaptable AI solutions. The setting appears to be a formal event with an audience, as indicated by the stage setup and the presence of a large screen displaying the topics being discussed.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5 - 80 to 99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The video features a presentation where a speaker, whose face is blurred, discusses advancements in technology. The speaker is seen on stage, delivering a talk about increasing efficiency, specifically mentioning \"2x tokens per minute,\" which suggests a focus on improving processing speed or capacity. Additionally, there is a segment showing a user interface for requesting a limit increase, specifically for the \"gpt-3.5-turbo\" model, indicating a discussion about enhancing or scaling up usage limits for a particular AI model. The presentation appears to be aimed at an audience interested in technological improvements and AI capabilities.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 - 100 to 119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The video appears to be a presentation about GPT-4 Turbo, focusing on its pricing and efficiency. The presenter discusses the cost benefits of GPT-4 Turbo, highlighting that it uses 3 times less input tokens and 2 times less output tokens compared to previous models. The presentation is part of an OpenAI event, as indicated by the \"OPENAI DEV DAY\" branding. The video also showcases a variety of applications or tools, possibly powered by GPT technology, displayed in a grid format. The term \"GPTs\" is emphasized, suggesting a focus on the capabilities or applications of GPT models.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 7 - 120 to 139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The video features a presentation where the speaker discusses a framework involving \"Instructions,\" \"Expanded knowledge,\" and \"Actions.\" The speaker emphasizes the concept of building with natural language, suggesting a focus on leveraging natural language processing or understanding in technology or software development. The presentation appears to be aimed at an audience interested in advancements in technology, possibly in the context of AI or machine learning.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 8 - 140 to 159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The video appears to be a presentation or keynote event, likely related to technology or software. It begins with a display of three icons on a screen, possibly representing different aspects or features of a product or service. A speaker, whose face is blurred, is seen on stage addressing an audience. The presentation includes a demonstration of a user interface, showcasing categories such as Programming, Data Analysis, Education, Lifestyle, and Just for Fun. Featured GPTs (Generative Pre-trained Transformers) are highlighted, including Reactify, Cumulus, Mid-century Future, and Autodeck, with a mention of DALL-E, an AI model for generating images. The video concludes with an animation of a rotating cube with the text \"OPENAI\" on its sides, suggesting the involvement of OpenAI in the presentation.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 - 160 to 179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The video appears to be a presentation from an OpenAI event, likely a developer day, as indicated by the \"OPENAI DEVDAY\" text. The speaker, whose face is blurred, is discussing various features related to APIs. The presentation highlights key concepts such as \"Threading,\" \"Retrieval,\" \"Code Interpreter,\" and \"Function Calling,\" which are displayed on a large screen behind the speaker. The event seems to be focused on introducing or explaining these technical features to an audience, likely developers or tech enthusiasts.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 - 180 to 199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The video appears to be a presentation at an event called \"OpenAI Dev Day.\" The speaker, whose face is blurred, is standing on a stage with a backdrop that repeatedly displays \"OPENAI DEV DAY.\" The speaker is dressed casually in a green sweater and dark pants, and is holding a clicker, suggesting they are giving a talk or presentation, possibly about developments or updates related to OpenAI. The audience is visible in silhouette, indicating a live event setting.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grab 20 frames and send them to GPT-4o for summarization\n",
    "# create a loop to grab every 20 frames and send them to OpenAI for summarization from base64Frames\n",
    "import copy \n",
    "\n",
    "nr_of_pics = len(base64Frames)\n",
    "nr_of_pics_per_round = 20\n",
    "start = 0\n",
    "end = nr_of_pics_per_round-1\n",
    "messages = []\n",
    "runs = 1\n",
    "accumulated_result = \"\"\n",
    "while end < nr_of_pics:\n",
    "    partial_frames = copy.deepcopy(base64Frames[start:end])\n",
    "    result = call_gpt4o_with_imgs(partial_frames)\n",
    "    print(f\"Run {runs} - {start} to {end}\")\n",
    "    display(result)\n",
    "    accumulated_result += result\n",
    "    if nr_of_pics - end < nr_of_pics_per_round:\n",
    "        break\n",
    "    start = end+1\n",
    "    end = start + nr_of_pics_per_round-1\n",
    "    runs += 1\n",
    "partial_frames = copy.deepcopy(base64Frames[end+1:nr_of_pics-1])\n",
    "result = call_gpt4o_with_imgs(partial_frames)\n",
    "accumulated_result += result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The video is a recap of the OpenAI DevDay event, highlighting the venue, keynote presentation, and various advancements in AI technology. The event features a bustling atmosphere with attendees gathered in a large conference hall. Key topics discussed include the introduction of GPT-4 Turbo, DALL-E 3, and advancements in Text-to-Speech technology. The presentation emphasizes improvements in AI capabilities, such as increased efficiency, enhanced processing speed, and the ability to handle complex tasks. The speaker, whose face is blurred, discusses features like JSON mode, function calling, and custom models, aimed at developers and tech enthusiasts. Visual elements include a demonstration of a user interface with categories like Programming and Data Analysis, and a focus on the cost benefits and efficiency of GPT-4 Turbo. The event concludes with the OpenAI logo, underscoring the company's role in these technological advancements.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final = call_gpt4o(accumulated_result)\n",
    "display(final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
